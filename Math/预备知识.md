# Pytorch预备知识

## 数据操作

使用某种方法来对数据进行存储和操作数据：

1.获取数据

2.将数据读入计算机后对其进行处理。

### 张量

n维数组，也称为 **张量(tensor)**, 与Python中的Numpy计算包操作及用法相似，再深度学习框架中，它的张量类都与Numpy的ndarray类似。深度学习框架比Numpy的ndarray的优势：

1.首先，能通过GPU进行加速计算，而Numpy仅仅支持CPU计算

2.其次，张量类支持自动微分，这些优势使得张量类更适合深度学习

张量表示由一个数值组成的数组，这个数组可能存在多个维度。具有一个轴的张量对应数学上的向量(vector)，具有两个轴的张量对应数学上的矩阵(matrix)，具有两个轴以上的张量没有特殊的数学名称。



1、⾸先，使⽤ arange 创建⼀个⾏向量 x。这个⾏向量包含以0开始的前12个整数，它们默认创建为整 数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 元素（element）。例如，张量 x 中有 12 个 元素。除⾮额外指定，新的张量将存储在内存中，并采⽤基于CPU的计算。 

```python
import torch
x = torch.arange(12)
print(x)
```

```python
tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
```

2、通过张量的shape属性来访问张量(沿每个轴的长度)形状

```python
print(x.shape)
```

```python
torch.Size([12])
```

3、计算张量中元素的总数，即形状的所有元素乘积，可检查大小(size)， **因为在此处理的是一个向量，所有shape与它的size相同** 

```python
print(x.numel())
```

```python
12
```

4、改变张量的形状，可调用reshape函数，但不会改变元素的数量和元素值。例如，将张量x从形状为(12, )的行行向量转换为形状为(3, 4)的矩阵。形状改变，但元素不会改变。通过改变张量的性质，张量大小不会改变

```python
X = x.reshape(3, 4)
print(X)
```

```python
tensor([
    [ 0, 1, 2, 3],
	[ 4, 5, 6, 7],
	[ 8, 9, 10, 11]
	])
```

5、通过手动指定每个维度来改变形状，如果知道目标形状为(高度, 宽度)，当知道其中之一的值之后，其他值将会被自动计算得出，可通过-1来调用此自动计算出维度的功能。如上手动指定3行4列，可通过x.reshape(-1, 4)或者x.reshape(3, -1)来进行自动计算。

```python
X = x.reshape(-1, 4)
print(X)
```

```python
tensor([
    [ 0, 1, 2, 3],
	[ 4, 5, 6, 7],
	[ 8, 9, 10, 11]
	])
```

### 运算符

在数学表⽰法中，将通过符号f : R → R​ 来表⽰⼀元标量运算符（只接收⼀个输⼊）。这意味着该函数从任何实数(R)射到另⼀个实数。

同样，通过符号 f : R, R → R 表⽰⼆元标量运算符，这意味着该函数接收两个输⼊，并产⽣⼀个输出。给定同⼀形状的任意两个向量u和v和⼆元运算符f，可以得到向量 c = F(u, v)。具体计算⽅法是 ci ← f(ui , vi)，其中ci, ui和vi分别是向量c、u和v中的元素。通过将标量函数升级为按元素向量运算来⽣成向量值 F : R d , R d → R d。  

```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y)
print(x - y)
print(x * y)
print(x / y)
print(x ** y) # **运算符是求幂运算
```

```python
(
    tensor([ 3., 4., 6., 10.]),
	tensor([-1., 0., 2., 6.]),
	tensor([ 2., 4., 8., 16.]),
	tensor([0.5000, 1.0000, 2.0000, 4.0000]),
	tensor([ 1., 4., 16., 64.])
)
```

### 广播机制

如何在相同形状的两个张量上执⾏按元素操作：

在某些情况下，即使形状不同， 仍然可以通过调⽤ ⼴播机制（broadcasting mechanism）来执⾏按元素操作。这种机制的⼯作⽅式如 下：

1.⾸先，通过适当复制元素来扩展⼀个或两个数组，以便在转换之后，两个张量具有相同的形状

2.其次，对 ⽣成的数组执⾏按元素操作



**何为广播**

1、沿着数组中长度为1的轴进行广播

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print(a)
print(b)
```

```python
(tensor([
    [0],
	[1],
	[2]]),
tensor([[0, 1]]))

```

由于a和b分别是 3 × 1​ 和 ​1 × 2​ 矩阵，如果让它们相加，它们的形状不匹配。将两个矩阵⼴播为⼀个更⼤的 ​3 × 2​矩阵，如下所⽰：矩阵a将复制列，矩阵b将复制⾏，然后再按元素相加。 

```python
print(a + b)
```

```python
tensor([
    [0, 1],
	[1, 2],
	[2, 3]]
	)
```

### 索引&切片

张量中的元素可通过索引来访问，与任何Python数组一样：

1.第一个元素的索引为0

2.最后一个元素的索引为-1

3.可指定范围以包含第一个元素和最后一个之前的元素

```python
print(X[-1], X[1:3])
```

```python
(tensor([ 8., 9., 10., 11.]),
tensor([[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.]]))
```

4、通过指定的索引将元素写入矩阵

```python
X[1, 2] = 9
print(X)
```

```python
tensor([[ 0., 1., 2., 3.],
[ 4., 5., 9., 7.],
[ 8., 9., 10., 11.]])
```

5、多个元素赋值相同的值，需要所有所有元素，然后为其赋值，如，[0:2, :]访问第1行和第2行

其中 ":" 代表沿轴1(列)的所有元素

```python
X[0:2, :] = 12
print(X)
```

```python
tensor([[12., 12., 12., 12.],
[12., 12., 12., 12.],
[ 8., 9., 10., 11.]])
```

### 节省内存

运行某些操作时，可能会导致新结果分配内存.

1、例如：Y = X + Y，将取消用Y指向的张量，而是指向新分配的内存处的张量，使用python的id()函数进行验证，它提供了内存中引用对象的确切地址，运算Y = x + Y 后，发现id(Y)指向另一个位置，因为python首先会计算 Y + X,为结果分配内存，然后使Y指向内存中的这个新位置。

```python
before = id(Y)
Y = Y + X
print(id(Y) == before)
```

```python
False
```

这可能是不可取的，原因有两个：⾸先，不想总是不必要地分配内存。在机器学习中，可能有数百兆的参数，并且在⼀秒内多次更新所有参数。通常情况下，希望原地执⾏这些更新。其次，如果不原地更新，其他引⽤仍然会指向旧的内存位置，这样某些代码可能会⽆意中引⽤旧的参数。  



2、执⾏原地操作⾮常简单。可以使⽤切⽚表⽰法将操作的结果分配给先前分配的数组，例 如Y[:] = 。为了说明这⼀点，⾸先创建⼀个新的矩阵Z，其形状与另⼀个Y相同， 使⽤zeros_like来分配⼀个全0的块。 

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))

Z[:] = X + Y
print('id(Z):', id(Z))
```

```python
id(Z): 140316199714544
id(Z): 140316199714544
```

3、 如果在后续计算中没有重复使⽤X，也可以使⽤X[:] = X + Y或X += Y来减少操作的内存开销。 

```python
before = id(X)
X += Y
print(id(X) == before)
```

```python
True
```

## 线性代数











